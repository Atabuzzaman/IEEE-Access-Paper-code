{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Libraries and package of python\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy import spatial\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import LineTokenizer \n",
    "from nltk.stem import PorterStemmer     \n",
    "tk = LineTokenizer()   \n",
    "lm = WordNetLemmatizer() \n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained Word2Vec model. It was pretrained on Google News Corpus\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sw=stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STS-2017 Dataset is used here.Here, we are taking the first sentence and second sentence into two list. \n",
    "fh=open('STS.input.track51.en-en.txt')\n",
    "c=0\n",
    "s1=[]\n",
    "s2=[]\n",
    "for line in fh:\n",
    "    line=line.lower()\n",
    "    c+=1\n",
    "    line=line.rstrip()\n",
    "    sentences=line.split('\\t')\n",
    "    s1.append(sentences[0])\n",
    "    s2.append(sentences[1]) \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum word-level Similarity(MWL_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('resultwordcommean1.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i].split(' ')\n",
    "    s1word=[]\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,'')\n",
    "        if word not in sw:\n",
    "            s1word.append(lemmatizer.lemmatize(word))\n",
    "    #print(s1word)\n",
    "    s2word=[]\n",
    "    ns2=s2[i].split(' ')\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            s2word.append(lemmatizer.lemmatize(word))\n",
    "    #print(s2word)\n",
    "    max_score=[]\n",
    "    for word in s1word:\n",
    "        if word in model.wv.vocab:\n",
    "            score=[]\n",
    "            for w in s2word:\n",
    "                if w in model.wv.vocab:\n",
    "                    score.append(model.similarity(word,w))\n",
    "        max_score.append(max(score))\n",
    "    #print(max_score)\n",
    "    \n",
    "    max_score2=[]\n",
    "    for word in s2word:\n",
    "        if word in model.wv.vocab:\n",
    "            score=[]\n",
    "            for w in s1word:\n",
    "                if w in model.wv.vocab:\n",
    "                    score.append(model.similarity(word,w))\n",
    "        max_score2.append(max(score))\n",
    "    #print(max_score2)\n",
    "    \n",
    "    count=0\n",
    "    summ=0\n",
    "    for val in max_score:\n",
    "        summ+=val\n",
    "        count+=1\n",
    "    average=(summ*5)/count\n",
    "    \n",
    "    count1=0\n",
    "    summ1=0\n",
    "    for val in max_score2:\n",
    "        summ1+=val\n",
    "        count1+=1\n",
    "    average1=(summ1*5)/count1\n",
    "    \n",
    "    aver=(average+average1)/2\n",
    "    #print('i:',i+1,aver)\n",
    "    a=str(round(aver,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    i+=1\n",
    "    \n",
    "file.close()\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('resultwordcommean1.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "mse = np.square(np.subtract(data2,data1)).mean()\n",
    "print('MSE:',mse)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Feature Vector based Similarity(V_avgS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('result1.txt','w')\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i].split(' ')\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    for word in ns1:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        #print(word)\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab): \n",
    "                sum1=np.add(sum1,model[word])\n",
    "    average1=np.divide(sum1,len(ns1),dtype=\"float32\")\n",
    "            \n",
    "    ns2=s2[i].split(' ')\n",
    "    sum2=np.zeros((300,),dtype=\"float32\")\n",
    "    for word in ns2:\n",
    "        for c in string.punctuation:\n",
    "                word=word.replace(c,\"\")\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                sum2=np.add(sum2,model[word])\n",
    "    average2=np.divide(sum2,len(ns2),dtype=\"float32\")\n",
    "    sim = 1 - spatial.distance.cosine(average1,average2)\n",
    "    sim=sim*5\n",
    "    a=str(round(sim,6))+'\\n'\n",
    "    file.writelines(a)\n",
    "    #print(i,'similarity',sim)\n",
    "    i+=1 \n",
    "file.close()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('result1.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum word-level similarity of NP, VP (MWL_NVP_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('resultNP_VP_compare123.txt','w')\n",
    "\n",
    "def partitioning(sentence):\n",
    "    sen=''\n",
    "    for word in sentence:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,\"\")\n",
    "        sen+=word\n",
    "    sentence=sen\n",
    "    doc=nlp(sen)\n",
    "    nounp=[]\n",
    "    verbp=[]\n",
    "    nounp=[chunk.text for chunk in doc.noun_chunks]\n",
    "    verbp=[token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    return nounp,verbp\n",
    "\n",
    "def listtostr(phrase):\n",
    "    sent=''\n",
    "    for word in phrase:\n",
    "        sent=sent+\" \"+str(word.strip())\n",
    "    return sent\n",
    "\n",
    "def result(s1,s2):\n",
    "    s1word=[]\n",
    "    s2word=[]\n",
    "    for word in s1:\n",
    "        if word not in sw:\n",
    "            s1word.append(word)\n",
    "    for word in s2:\n",
    "        if word not in sw:\n",
    "            s2word.append(word)\n",
    "            \n",
    "    #print(s1word,s2word)\n",
    "    if len(s1word)==0 or len(s2word)==0:\n",
    "        aver=0\n",
    "    else:\n",
    "        max_score=[]\n",
    "        for word in s1word:\n",
    "            score=[0]\n",
    "            if word in model.wv.vocab:\n",
    "                for w in s2word:\n",
    "                    if w in model.wv.vocab:\n",
    "                        score.append(model.similarity(word,w))\n",
    "            max_score.append(max(score))\n",
    "        #print(max_score)\n",
    "    \n",
    "        max_score2=[]\n",
    "        for word in s2word:\n",
    "            score=[0]\n",
    "            if word in model.wv.vocab:\n",
    "                for w in s1word:\n",
    "                    if w in model.wv.vocab:\n",
    "                        score.append(model.similarity(word,w))\n",
    "            max_score2.append(max(score))\n",
    "        #print(max_score2)\n",
    "    \n",
    "        average=sum(max_score)/(len(max_score))\n",
    "    \n",
    "        average1=sum(max_score2)/(len(max_score2))\n",
    "    \n",
    "        aver=(average+average1)/2\n",
    "    return aver\n",
    "\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i]\n",
    "    #print(ns1)\n",
    "    npp,vpp=partitioning(ns1)\n",
    "    #print(npp,vpp)\n",
    "    sen1np=listtostr(npp).strip()\n",
    "    sen1np=sen1np.split(' ')\n",
    "    #print(sen1np, len(sen1np))\n",
    "    sen1vp=listtostr(vpp).strip()\n",
    "    sen1vp=sen1vp.split(' ')\n",
    "        \n",
    "    ns2=s2[i]\n",
    "    #print(ns2)\n",
    "    npp,vpp=partitioning(ns2)\n",
    "    #print(npp,vpp)\n",
    "    sen2np=listtostr(npp).strip()\n",
    "    sen2np=sen2np.split(' ')\n",
    "    sen2vp=listtostr(vpp).strip()\n",
    "    sen2vp=sen2vp.split(' ')\n",
    "    \n",
    "    avernp=result(sen1np,sen2np)\n",
    "\n",
    "    avervp=result(sen1vp,sen2vp)\n",
    "    \n",
    "    av=avernp*4+avervp\n",
    "    av=str(round(av,6))+'\\n'\n",
    "    #print('No:',i+1,'similarity:',av)\n",
    "    file.writelines(av)\n",
    "    \n",
    "    i=i+1\n",
    "    \n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('resultNP_VP_compare123.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "mse=np.square(np.subtract(data2,data1)).mean()\n",
    "print(\"MSE:\",mse)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase Structured-based Similarity(Feature Vector based (AFS_NVP_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('resultNP_VP12.txt','w')\n",
    "\n",
    "def partitioning(sentence):\n",
    "    sen=''\n",
    "    for word in sentence:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,\"\")\n",
    "        sen+=word\n",
    "    sentence=sen\n",
    "    doc=nlp(sen)\n",
    "    nounp=[]\n",
    "    verbp=[]\n",
    "    nounp=[chunk.text for chunk in doc.noun_chunks]\n",
    "    verbp=[token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    return nounp,verbp\n",
    "\n",
    "def vectorvalue(subsent):\n",
    "    #print(subsent)\n",
    "    subsent=subsent.split(\" \")\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    for word in subsent:\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                sum1=np.add(sum1,model[word])\n",
    "    if len(subsent)>0:\n",
    "        average1=np.divide(sum1,len(subsent),dtype=\"float32\")\n",
    "    else:\n",
    "        average1=sum1\n",
    "    return average1\n",
    "\n",
    "def listtostr(phrase):\n",
    "    sent=''\n",
    "    for word in phrase:\n",
    "        sent=sent+\" \"+str(word)\n",
    "    return vectorvalue(sent)\n",
    "\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i]\n",
    "    #print(ns1)\n",
    "    npp,vpp=partitioning(ns1)\n",
    "    #print(npp,vpp)\n",
    "    sen1np=listtostr(npp)\n",
    "    sen1vp=listtostr(vpp)\n",
    "        \n",
    "    ns2=s2[i]\n",
    "    #print(ns2)\n",
    "    npp,vpp=partitioning(ns2)\n",
    "    #print(npp,vpp)\n",
    "    sen2np=listtostr(npp)\n",
    "    sen2vp=listtostr(vpp)\n",
    "        \n",
    "    if ((np.all(sen1np==0)) or (np.all(sen2np==0))):\n",
    "            nppsim=0\n",
    "    else:\n",
    "        nppsim= 1 - spatial.distance.cosine(sen1np,sen2np)\n",
    "\n",
    "    if ((np.all(sen1vp==0)) or (np.all(sen2vp==0))):\n",
    "            vppsim=0\n",
    "    else:\n",
    "        vppsim=1 - spatial.distance.cosine(sen1vp,sen2vp)\n",
    "    similarity=nppsim*4+vppsim\n",
    "    #print(\"No:\",i,similarity)\n",
    "    sim=str(round(similarity,2))+'\\n'\n",
    "    file.writelines(sim)\n",
    "    i+=1\n",
    "    \n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('resultNP_VP12.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "mse = np.square(np.subtract(data2,data1)).mean()\n",
    "print('MSE:',mse)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase structure based Sim_NVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "sw=stop_words\n",
    "#print(sw)\n",
    "fh=open('STS.input.track51.en-en.txt')\n",
    "c=0\n",
    "s1=[]\n",
    "s2=[]\n",
    "for line in fh:\n",
    "    line=line.lower()\n",
    "    c+=1\n",
    "    line=line.rstrip()\n",
    "    sentences=line.split('\\t')\n",
    "    s1.append(sentences[0])\n",
    "    s2.append(sentences[1])    \n",
    "    #print(s1,s2)\n",
    "print(c)\n",
    "\n",
    "def partitioning(sentence):\n",
    "    sen=''\n",
    "    for word in sentence:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,\"\")\n",
    "        sen+=word\n",
    "    sentence=sen\n",
    "    doc=nlp(sen)\n",
    "    nounp=[]\n",
    "    verbp=[]\n",
    "    nounp=[chunk.text for chunk in doc.noun_chunks]\n",
    "    verbp=[token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    return nounp,verbp\n",
    "\n",
    "file=open('resultNP.txt','w')\n",
    "def wnb(s1,s2):\n",
    "    from nltk.corpus import wordnet as wn\n",
    "    w1=[]\n",
    "    w2=[]\n",
    "    \n",
    "    words1 = word_tokenize(s1)\n",
    "    for word in words1:\n",
    "        if word not in sw:\n",
    "            w1.append(word)\n",
    "    #print(\"word1:\",w1)\n",
    "    \n",
    "    words2= word_tokenize(s2)\n",
    "    for word in words2:\n",
    "        if word not in sw:\n",
    "            w2.append(word)\n",
    "    #print(\"word2:\",w2)\n",
    "    w11=[]\n",
    "    w22=[]\n",
    "    for w in w1:\n",
    "        try:\n",
    "            #print(wn.synsets(w)[0])\n",
    "            w11.append(wn.synsets(w)[0])\n",
    "        except:\n",
    "            continue\n",
    "    for w in w2:\n",
    "        try:\n",
    "            #print(wn.synsets(w)[0])\n",
    "            w22.append(wn.synsets(w)[0])\n",
    "        except:\n",
    "            continue\n",
    "    x=[]\n",
    "    for w in w11:\n",
    "        s=[]\n",
    "        for se in w22:\n",
    "            try:\n",
    "                sim=w.path_similarity(se)\n",
    "                #print(sim)\n",
    "                if(sim==None):\n",
    "                    s.append(0)\n",
    "                else:\n",
    "                    s.append(sim)\n",
    "            except:\n",
    "                continue\n",
    "        if (len(s)>0):\n",
    "            x.append(max(s))\n",
    "        #print(x)\n",
    "    if(len(x)>0):\n",
    "        si=sum(x)/len(x)\n",
    "    else:\n",
    "        si=0\n",
    "    return si\n",
    "\n",
    "def listtostr(phrase):\n",
    "    sent=''\n",
    "    for word in phrase:\n",
    "        sent=sent+\" \"+str(word)\n",
    "    return sent\n",
    "\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i]\n",
    "   # print(ns1)\n",
    "    npp,vpp=partitioning(ns1)\n",
    "   # print(npp,vpp)\n",
    "    sen1np=listtostr(npp)\n",
    "    sen1vp=listtostr(vpp)\n",
    "        \n",
    "    ns2=s2[i]\n",
    "    #print(ns2)\n",
    "    npp,vpp=partitioning(ns2)\n",
    "    #print(npp,vpp)\n",
    "    sen2np=listtostr(npp)\n",
    "    sen2vp=listtostr(vpp)\n",
    "        \n",
    "    nppsim=float(wnb(sen1np,sen2np))\n",
    "   # print(\"np:\", nppsim)\n",
    "    vppsim=float(wnb(sen1vp,sen2vp))\n",
    "    #print(\"vp:\", vppsim)\n",
    "    \n",
    "     \n",
    "    if(nppsim==0 or vppsim==0):\n",
    "        similarity=(nppsim+vppsim)*5\n",
    "    elif(nppsim !=0 and vppsim !=0):\n",
    "        similarity= nppsim*4+vppsim\n",
    "    #print(\"No:\",i+1,similarity)\n",
    "    sim=str(round(similarity,2))+'\\n'\n",
    "    file.writelines(sim)\n",
    "    i+=1\n",
    "    \n",
    "file.close()\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f1=open('resultNP.txt')\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "mse=np.square(np.subtract(data2,data1)).mean()\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Role-based Maximum similarity RM_Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('newNpVp.txt','w')\n",
    "\n",
    "def partitioning(sentence):\n",
    "    sen=''\n",
    "    for word in sentence:\n",
    "        for c in string.punctuation:\n",
    "            word=word.replace(c,\"\")\n",
    "        sen+=word\n",
    "    sentence=sen\n",
    "    doc=nlp(sen)\n",
    "    nounp=[]\n",
    "    verbp=[]\n",
    "    nounp=[chunk.text for chunk in doc.noun_chunks]\n",
    "    verbp=[token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    return nounp,verbp\n",
    "\n",
    "def vectorvalue(subsent):\n",
    "    #print(subsent)\n",
    "    subsent=subsent.split(\" \")\n",
    "    sum1=np.zeros((300,), dtype=\"float32\")\n",
    "    for word in subsent:\n",
    "        if word not in sw:\n",
    "            if(word in model.wv.vocab):\n",
    "                sum1=np.add(sum1,model[word])\n",
    "    if len(subsent)>0:\n",
    "        average1=np.divide(sum1,len(subsent),dtype=\"float32\")\n",
    "    else:\n",
    "        average1=sum1\n",
    "    return average1\n",
    "\n",
    "def listvalue(pp):\n",
    "    pplist=[]\n",
    "    if(len(pp)>0):\n",
    "        j=0\n",
    "        while(j<len(pp)):\n",
    "            pplist.append(vectorvalue(pp[j]))\n",
    "            j=j+1\n",
    "    else:\n",
    "        pplist.append(np.zeros((300,), dtype=\"float32\"))\n",
    "    return pplist\n",
    "\n",
    "def score(nplist,np2list,vplist,vp2list,a):\n",
    "    simlist=[]\n",
    "    if(np.all(nplist)==0 and np.all(np2list)==0 and np.all(vplist)==0 and np.all(vp2list)==0):\n",
    "        aa=0\n",
    "    else:\n",
    "        for value in nplist:\n",
    "            npsim=[]\n",
    "            for value2 in np2list:\n",
    "                sim=(1 - spatial.distance.cosine(value,value2))*(5/a)\n",
    "                if( np.isnan(sim)==1):\n",
    "                    sim=0\n",
    "                npsim.append(sim)\n",
    "            #print('np:',max(npsim))\n",
    "            simlist.append(max(npsim))\n",
    "        \n",
    "        for v in vplist:\n",
    "            vpsim=[]\n",
    "            for v2 in vp2list:\n",
    "                sim=(1-spatial.distance.cosine(v,v2))*(5/a)\n",
    "                if np.isnan(sim)==1:\n",
    "                    sim=0\n",
    "                vpsim.append(sim)\n",
    "            #print('vp:',max(vpsim))\n",
    "            simlist.append(max(vpsim))\n",
    "        if(len(simlist)>0):    \n",
    "            bb=max(simlist)\n",
    "            p=0\n",
    "            while(p<len(simlist)):\n",
    "                if simlist[p]<bb/2:\n",
    "                    simlist[p]=0\n",
    "                p=p+1\n",
    "        aa = sum(simlist)\n",
    "    return aa\n",
    "\n",
    "ns1=[]\n",
    "ns2=[]\n",
    "i=0\n",
    "print('Length:',len(s1))\n",
    "while(i<len(s1)):\n",
    "    ns1=s1[i]\n",
    "    #print(ns1)\n",
    "    nplist=[]\n",
    "    vplist=[]\n",
    "    np2list=[]\n",
    "    vp2list=[]\n",
    "    npp,vpp=partitioning(ns1)\n",
    "    #print(npp,vpp)\n",
    "    nplist=listvalue(npp)\n",
    "    vplist=listvalue(vpp)\n",
    "    a=len(npp)+len(vpp)\n",
    "    \n",
    "    ns2=s2[i]\n",
    "    #print(ns2)\n",
    "    npp2,vpp2=partitioning(ns2)\n",
    "    #print(npp2,vpp2)\n",
    "    np2list=listvalue(npp2)\n",
    "    vp2list=listvalue(vpp2)\n",
    "    b=len(npp2)+len(vpp2)\n",
    "    \n",
    "    if a>=b:\n",
    "        sim=score(nplist,np2list,vplist,vp2list,a)\n",
    "    else:\n",
    "        sim=score(np2list,nplist,vp2list,vplist,b)\n",
    "    #print(i+1,'similarity:',sim)\n",
    "    sim=str(round(sim,2))+'\\n'\n",
    "    file.writelines(sim)\n",
    "    i=i+1\n",
    "    \n",
    "file.close()\n",
    "\n",
    "data1=[]\n",
    "data2=[]\n",
    "f2=open('STS.gs.track5.en-en.txt')\n",
    "f1=open('newNpVp.txt')\n",
    "#f2=open('newNpVp.txt')\n",
    "\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line)/5)\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "print(len(data1),len(data2))\n",
    "mse = np.square(np.subtract(data2,data1)).mean()\n",
    "print('MSE:',mse)\n",
    "corr,_=pearsonr(data1,data2)\n",
    "scor,_=spearmanr(data1,data2)\n",
    "print('Pearson:',corr*100,'Spearman:',scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of Phrase Structured based measures\n",
    "f1=open('resultNP_VP_compare12.txt')\n",
    "f2=open('resultNP_VP12.txt')\n",
    "f3=open('newNpVp.txt')\n",
    "f4=open('resultNP.txt')\n",
    "f5=open('STS.gs.track5.en-en.txt')\n",
    "file=open('combinephrase.txt','w')\n",
    "data1=[]\n",
    "data2=[]\n",
    "data3=[]\n",
    "data4=[]\n",
    "data5=[]\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "data3=listvalue(f3)\n",
    "data4=listvalue(f4)\n",
    "data5=listvalue(f5)\n",
    "aver=[]\n",
    "i=0\n",
    "while(i<len(data1)):\n",
    "    data=(data1[i]+data2[i]+data3[i]+data4[i])/4\n",
    "    aver.append(data)\n",
    "    data=str(round(data,1))+'\\n'\n",
    "    file.writelines(data)\n",
    "    i=i+1\n",
    "corr,_=pearsonr(data5,aver)\n",
    "scor,_=spearmanr(data5,aver)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of without phrase structured measures\n",
    "data1=[]\n",
    "data2=[]\n",
    "data3=[]\n",
    "f1=open('result1.txt')\n",
    "f2=open('resultwordcommean1.txt')\n",
    "f3=open('STS.gs.track5.en-en.txt')\n",
    "file=open(\"combineword.txt\",'w')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "data3=listvalue(f3)\n",
    "#print(len(data1),len(data2),len(data3))\n",
    "i=0\n",
    "aver1=[]\n",
    "while(i<len(data1)):\n",
    "    data=(data1[i]+data2[i])/2\n",
    "    aver1.append(data)\n",
    "    data=str(round(data,1))+'\\n'\n",
    "    file.writelines(data)\n",
    "    i=i+1\n",
    "corr,_=pearsonr(data3,aver1)\n",
    "scor,_=spearmanr(data3,aver1)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of all measures\n",
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "d4=[]\n",
    "f1=open('combineword.txt')\n",
    "f2=open('combinephrase.txt')\n",
    "f3=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "d1=listvalue(f1)\n",
    "d2=listvalue(f2)\n",
    "d3=listvalue(f3)\n",
    "i=0\n",
    "while i<len(d1):\n",
    "    d4.append((d1[i]+d2[i])/2)\n",
    "    i=i+1\n",
    "corr,_=pearsonr(d3,d4)\n",
    "scor,_=spearmanr(d3,d4)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the End of our Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of Phrase Structured based measures without RM_Sim\n",
    "f1=open('resultNP_VP_compare12.txt')\n",
    "f2=open('resultNP_VP12.txt')\n",
    "#f3=open('newNpVp.txt')\n",
    "f4=open('resultNP.txt')\n",
    "f5=open('STS.gs.track5.en-en.txt')\n",
    "file=open('combinephrase3.txt','w')\n",
    "data1=[]\n",
    "data2=[]\n",
    "data3=[]\n",
    "data4=[]\n",
    "data5=[]\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "#data3=listvalue(f3)\n",
    "data4=listvalue(f4)\n",
    "data5=listvalue(f5)\n",
    "aver=[]\n",
    "i=0\n",
    "while(i<len(data1)):\n",
    "    data=(data1[i]+data2[i]+data4[i])/3\n",
    "    aver.append(data)\n",
    "    data=str(round(data,1))+'\\n'\n",
    "    file.writelines(data)\n",
    "    i=i+1\n",
    "corr,_=pearsonr(data5,aver)\n",
    "scor,_=spearmanr(data5,aver)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of all measures except RM_Sim\n",
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "d4=[]\n",
    "f1=open('combineword.txt')\n",
    "f2=open('combinephrase3.txt')\n",
    "f3=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "d1=listvalue(f1)\n",
    "d2=listvalue(f2)\n",
    "d3=listvalue(f3)\n",
    "i=0\n",
    "while i<len(d1):\n",
    "    d4.append((d1[i]+d2[i])/2)\n",
    "    i=i+1\n",
    "corr,_=pearsonr(d3,d4)\n",
    "scor,_=spearmanr(d3,d4)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of Phrase Structured based measures without phrase word-level similarity\n",
    "#f1=open('resultNP_VP_compare12.txt')\n",
    "f2=open('resultNP_VP12.txt')\n",
    "f3=open('newNpVp.txt')\n",
    "f4=open('resultNP.txt')\n",
    "f5=open('STS.gs.track5.en-en.txt')\n",
    "file=open('combinephrase3.txt','w')\n",
    "data1=[]\n",
    "data2=[]\n",
    "data3=[]\n",
    "data4=[]\n",
    "data5=[]\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "#data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "data3=listvalue(f3)\n",
    "data4=listvalue(f4)\n",
    "data5=listvalue(f5)\n",
    "aver=[]\n",
    "i=0\n",
    "while(i<len(data5)):\n",
    "    data=(data3[i]+data2[i]+data4[i])/3\n",
    "    aver.append(data)\n",
    "    data=str(round(data,1))+'\\n'\n",
    "    file.writelines(data)\n",
    "    i=i+1\n",
    "print(len(aver))\n",
    "corr,_=pearsonr(data5,aver)\n",
    "scor,_=spearmanr(data5,aver)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of all measures except Phrase word-level\n",
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "d4=[]\n",
    "f1=open('combineword.txt')\n",
    "f2=open('combinephrase3.txt')\n",
    "f3=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "d1=listvalue(f1)\n",
    "d2=listvalue(f2)\n",
    "d3=listvalue(f3)\n",
    "\n",
    "i=0\n",
    "while i<len(d1):\n",
    "    d4.append((d1[i]+d2[i])/2)\n",
    "    i=i+1\n",
    "corr,_=pearsonr(d3,d4)\n",
    "scor,_=spearmanr(d3,d4)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of Phrase Structured based measures without phrase average vector\n",
    "f1=open('resultNP_VP_compare12.txt')\n",
    "#f2=open('resultNP_VP12.txt')\n",
    "f3=open('newNpVp.txt')\n",
    "f4=open('resultNP.txt')\n",
    "f5=open('STS.gs.track5.en-en.txt')\n",
    "file=open('combinephrase3.txt','w')\n",
    "data1=[]\n",
    "data2=[]\n",
    "data3=[]\n",
    "data4=[]\n",
    "data5=[]\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "data3=listvalue(f3)\n",
    "data4=listvalue(f4)\n",
    "data5=listvalue(f5)\n",
    "aver=[]\n",
    "i=0\n",
    "while(i<len(data5)):\n",
    "    data=(data3[i]+data1[i]+data4[i])/3\n",
    "    aver.append(data)\n",
    "    data=str(round(data,1))+'\\n'\n",
    "    file.writelines(data)\n",
    "    i=i+1\n",
    "print(len(aver))\n",
    "corr,_=pearsonr(data5,aver)\n",
    "scor,_=spearmanr(data5,aver)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of all measures except phrase average vector\n",
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "d4=[]\n",
    "f1=open('combineword.txt')\n",
    "f2=open('combinephrase3.txt')\n",
    "f3=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "d1=listvalue(f1)\n",
    "d2=listvalue(f2)\n",
    "d3=listvalue(f3)\n",
    "i=0\n",
    "while i<len(d1):\n",
    "    d4.append((d1[i]+d2[i])/2)\n",
    "    i=i+1\n",
    "corr,_=pearsonr(d3,d4)\n",
    "scor,_=spearmanr(d3,d4)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of Phrase Structured based measures without WN POS similarity\n",
    "f1=open('resultNP_VP_compare12.txt')\n",
    "f2=open('resultNP_VP12.txt')\n",
    "f3=open('newNpVp.txt')\n",
    "#f4=open('resultNP.txt')\n",
    "f5=open('STS.gs.track5.en-en.txt')\n",
    "file=open('combinephrase3.txt','w')\n",
    "data1=[]\n",
    "data2=[]\n",
    "data3=[]\n",
    "data4=[]\n",
    "data5=[]\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "data3=listvalue(f3)\n",
    "#data4=listvalue(f4)\n",
    "data5=listvalue(f5)\n",
    "aver=[]\n",
    "i=0\n",
    "while(i<len(data5)):\n",
    "    data=(data3[i]+data2[i]+data1[i])/3\n",
    "    aver.append(data)\n",
    "    data=str(round(data,1))+'\\n'\n",
    "    file.writelines(data)\n",
    "    i=i+1\n",
    "print(len(aver))\n",
    "corr,_=pearsonr(data5,aver)\n",
    "scor,_=spearmanr(data5,aver)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of all measures except phrase WN POS\n",
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "d4=[]\n",
    "f1=open('combineword.txt')\n",
    "f2=open('combinephrase3.txt')\n",
    "f3=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "d1=listvalue(f1)\n",
    "d2=listvalue(f2)\n",
    "d3=listvalue(f3)\n",
    "i=0\n",
    "while i<len(d1):\n",
    "    d4.append((d1[i]+d2[i])/2)\n",
    "    i=i+1\n",
    "corr,_=pearsonr(d3,d4)\n",
    "scor,_=spearmanr(d3,d4)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of all measures except maximum word-level similarity\n",
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "d4=[]\n",
    "f1=open('result1.txt')\n",
    "f2=open('combinephrase2.txt')\n",
    "f3=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "d1=listvalue(f1)\n",
    "d2=listvalue(f2)\n",
    "\n",
    "d3=listvalue(f3)\n",
    "i=0\n",
    "while i<len(d1):\n",
    "    d4.append((d1[i]+d2[i])/2)\n",
    "    i=i+1\n",
    "corr,_=pearsonr(d3,d4)\n",
    "scor,_=spearmanr(d3,d4)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of all measures except average feature vector\n",
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "d4=[]\n",
    "f1=open('resultwordcommean1.txt')\n",
    "f2=open('combinephrase2.txt')\n",
    "f3=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "d1=listvalue(f1)\n",
    "d2=listvalue(f2)\n",
    "\n",
    "d3=listvalue(f3)\n",
    "i=0\n",
    "while i<len(d1):\n",
    "    d4.append((d1[i]+d2[i])/2)\n",
    "    i=i+1\n",
    "corr,_=pearsonr(d3,d4)\n",
    "scor,_=spearmanr(d3,d4)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of Phrase Structured based measures without WN POS similarity and RM_sim\n",
    "f1=open('resultNP_VP_compare12.txt')\n",
    "f2=open('resultNP_VP12.txt')\n",
    "#f3=open('newNpVp.txt')\n",
    "#f4=open('resultNP.txt')\n",
    "f5=open('STS.gs.track5.en-en.txt')\n",
    "file=open('combinephrase3.txt','w')\n",
    "data1=[]\n",
    "data2=[]\n",
    "data3=[]\n",
    "data4=[]\n",
    "data5=[]\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "data1=listvalue(f1)\n",
    "data2=listvalue(f2)\n",
    "#data3=listvalue(f3)\n",
    "#data4=listvalue(f4)\n",
    "data5=listvalue(f5)\n",
    "aver=[]\n",
    "i=0\n",
    "while(i<len(data1)):\n",
    "    data=(data2[i]+data1[i])/2\n",
    "    aver.append(data)\n",
    "    data=str(round(data,1))+'\\n'\n",
    "    file.writelines(data)\n",
    "    i=i+1\n",
    "print(len(aver))\n",
    "corr,_=pearsonr(data5,aver)\n",
    "scor,_=spearmanr(data5,aver)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of all measures except phrase WN POS and RM_sim\n",
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "d4=[]\n",
    "f1=open('combineword.txt')\n",
    "f2=open('combinephrase3.txt')\n",
    "f3=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "d1=listvalue(f1)\n",
    "d2=listvalue(f2)\n",
    "d3=listvalue(f3)\n",
    "i=0\n",
    "while i<len(d1):\n",
    "    d4.append((d1[i]+d2[i])/2)\n",
    "    i=i+1\n",
    "corr,_=pearsonr(d3,d4)\n",
    "scor,_=spearmanr(d3,d4)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "d4=[]\n",
    "f1=open('combineword.txt')\n",
    "f2=open('combinephrase.txt')\n",
    "f3=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "d1=listvalue(f1)\n",
    "d2=listvalue(f2)\n",
    "d3=listvalue(f3)\n",
    "print(len(d1),len(d2),len(d3))\n",
    "i=0\n",
    "while i<len(d1):\n",
    "    if d1[i]<d2[i]:\n",
    "        d4.append((d1[i]+d2[i])/2)\n",
    "    else:\n",
    "        d4.append(d1[i])\n",
    "    i=i+1\n",
    "corr,_=pearsonr(d3,d4)\n",
    "scor,_=spearmanr(d3,d4)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of all measures\n",
    "d1=[]\n",
    "d2=[]\n",
    "d3=[]\n",
    "d4=[]\n",
    "f1=open('combineword.txt')\n",
    "f2=open('combinephrase.txt')\n",
    "f3=open('STS.gs.track5.en-en.txt')\n",
    "def listvalue(file):\n",
    "    value=[]\n",
    "    for line in file:\n",
    "        value.append(float(line))\n",
    "    return value\n",
    "d1=listvalue(f1)\n",
    "d2=listvalue(f2)\n",
    "d3=listvalue(f3)\n",
    "i=0\n",
    "while i<len(d1):\n",
    "    d4.append((d1[i]+d2[i])/2)\n",
    "    i=i+1\n",
    "corr,_=pearsonr(d3,d4)\n",
    "scor,_=spearmanr(d3,d4)\n",
    "print(\"Pearson's:\",corr*100)\n",
    "print(\"Spearman's:\",scor*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "c=0\n",
    "while i<len(d3):\n",
    "    a= d3[i]-d4[i]\n",
    "    b=d4[i]-d3[i]\n",
    "    #if(-0.1<a<0.1):\n",
    "    if((a > 2.5) or (b > 2.5)):\n",
    "        print('NO:',i+1,'GS:', d3[i],'Our Method:', d4[i])\n",
    "        c=c+1\n",
    "    i=i+1\n",
    "print('count:',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "workbook   = xlsxwriter.Workbook('score_comparison.xlsx')\n",
    "\n",
    "worksheet1 = workbook.add_worksheet()\n",
    "worksheet2 = workbook.add_worksheet()\n",
    "\n",
    "i=0\n",
    "while i<250:\n",
    "    worksheet1.write_number(i,0, d3[i])\n",
    "    worksheet1.write_number(i, 1, d4[i])\n",
    "    i=i+1\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
